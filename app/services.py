def get_ai_response(
    llm_client, onet_collection, user_question, profile_summary, model, embedding_model
):
    """
    Retrieves relevant documents from the database based on the user's question,
    creates a prompt, and asks the LLM to generate an answer.

    Args:
        llm_client: Initialized OpenAI client.
        onet_collection: Initialized ChromaDB Collection.
        user_question (str): The user's question.
        profile_summary (str): The user's RIASEC profile summary.
        model (str): Name of the LLM model to use.
        embedding_model: SentenceTransformer model for embedding.

    Raises:
        ValueError: If an error occurs during service calls.

    Returns:
        str: The answer generated by the AI model.
    """
    # Retrieve relevant documents from the vector database
    try:
        query_vector = embedding_model.encode([user_question]).tolist()[0]
        results = onet_collection.query(
            query_embeddings=[query_vector],
            n_results=5,
            include=["documents", "metadatas"],
        )

        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
        retrieved_chunks = []
        for doc, meta in zip(docs, metas):
            title = meta.get("title", "Untitled")
            retrieved_chunks.append(f"{title}: {doc}")

        retrieved_docs = "\n\n---\n\n".join(retrieved_chunks)
    except Exception as e:
        print(f"ChromaDB search error: {e}")
        raise ValueError(
            "Could not retrieve relevant documents from the knowledge base."
        )

    # Create system and user prompts for the LLM
    system_prompt = (
        "You are 'OccumendAI', an expert and empathetic career strategist. "
        "Your primary goal is to help the user understand their RIASEC profile and explore potential career paths in a thoughtful and empowering way. "
        "You are not a simple Q&A bot; you are a guide."
        "\n\n"
        "### Core Directives:\n"
        "1.  **Persona & Tone**: Be professional, encouraging, and insightful. Use a positive tone that builds the user's confidence. Address the user directly and respectfully."
        "2.  **Grounding is Critical**: Base ALL your answers strictly on the user's profile summary and the O*NET job documents provided in the context. Explicitly reference the user's RIASEC scores (e.g., 'Your high score in Enterprising suggests...') and the provided job data. DO NOT invent information or provide details about jobs not included in the context."
        "3.  **Synthesize, Don't Just List**: Do not just repeat the information given to you. Your value lies in connecting the dots. Explain *why* a certain job fits (or doesn't fit) the user's profile by linking specific job tasks or work environments to their RIASEC interests."
        "4.  **Structure and Formatting**: Structure your answers for maximum clarity. Use simple HTML tags: `<h3>` for main sections, `<strong>` for emphasis, and `<ul>` with `<li>` for lists. Keep paragraphs concise."
        "5.  **Maintain Dialogue**: Always end your response with a thoughtful, open-ended question to encourage further exploration and keep the conversation going. For example, 'Which of these aspects sounds most appealing to you?' or 'Would you like to dive deeper into the daily tasks of a Landscape Architect?'"
        "\n\n"
        "### Boundaries:\n"
        "- You are NOT a life coach or a therapist. Avoid giving psychological advice."
        "- You do NOT guarantee job placement or salary outcomes."
        "- You do NOT provide information outside of the career context (e.g., financial advice, personal opinions)."
    )

    human_prompt = (
        f"USER PROFILE: {profile_summary}\n\n"
        f"O*NET JOB DOCUMENTS:\n"
        f"---------------------\n"
        f"{retrieved_docs}\n"
        f"---------------------\n\n"
        f"Based on all the above, answer my question: '{user_question}'"
    )

    # Call the LLM to generate the answer
    try:
        response = llm_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": human_prompt},
            ],
            temperature=0.7,
            max_tokens=2000,
        )
        answer = response.choices[0].message.content
        return answer
    except Exception as e:
        print(f"LLM API call error: {e}")
        raise ValueError("An error occurred while communicating with the AI model.")
